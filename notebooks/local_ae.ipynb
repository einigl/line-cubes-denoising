{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally Connected Autoencoder Neural Network based denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "\n",
    "from torch import nn, optim, no_grad\n",
    "\n",
    "from line_cube_denoising.dataset import CubeDataset\n",
    "from line_cube_denoising.networks import Autoencoder, LocalAutoencoder\n",
    "from line_cube_denoising.training import ReconstructionLoss, InformedLoss, LearningParameters, learning_procedure\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), \"..\", \"cubes\")\n",
    "results_path = os.path.join(os.getcwd(), \"out_dense_ae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataset\n",
    "\n",
    "line_name = \"12cn10_small\"\n",
    "\n",
    "hdu = fits.open(os.path.join(data_path, \"preprocessing\", line_name, f\"{line_name}.fits\"))[0]\n",
    "cube = hdu.data\n",
    "header = hdu.header\n",
    "\n",
    "mask_cube = fits.open(os.path.join(data_path, \"preprocessing\", line_name, f\"{line_name}_mask.fits\"))[0].data\n",
    "noise_map = fits.open(os.path.join(data_path, \"preprocessing\", line_name, f\"{line_name}_noise_map.fits\"))[0].data\n",
    "\n",
    "cube = cube.astype(np.float32)\n",
    "mask_cube = mask_cube.astype(np.float32)\n",
    "noise_map = noise_map.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created: 80 features, 141050 samples\n"
     ]
    }
   ],
   "source": [
    "channels_range = (None, None)\n",
    "# n_inputs = channels_range[1] - channels_range[0]\n",
    "n_inputs = cube.shape[0]\n",
    "\n",
    "dataset = CubeDataset(\n",
    "    cube[slice(*channels_range)],\n",
    "    mask_cube[slice(*channels_range)],\n",
    "    noise_map\n",
    ")\n",
    "\n",
    "nx, ny, nz = dataset.nx, dataset.ny, dataset.nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the architecture\n",
    "\n",
    "win_size = 5\n",
    "bottleneck_size = round(0.75*n_inputs)\n",
    "half_description_by_win = [n_inputs, 5*n_inputs, 5*n_inputs, 5*n_inputs]\n",
    "activation = nn.ELU()\n",
    "\n",
    "model = LocalAutoencoder(\n",
    "    n_inputs,\n",
    "    bottleneck_size,\n",
    "    win_size,\n",
    "    half_description_by_win,\n",
    "    activation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "normalize = True\n",
    "# loss = ReconstructionLoss(normalize=normalize)\n",
    "loss = InformedLoss(normalize=normalize)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 500\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "gamma = 0.95\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n",
    "\n",
    "learning_parameters = LearningParameters(\n",
    "    loss,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    optimizer,\n",
    "    scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x "
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "\n",
    "val_frac = 0.2\n",
    "seed = None\n",
    "\n",
    "results = learning_procedure(\n",
    "    model,\n",
    "    dataset,\n",
    "    learning_parameters,\n",
    "    val_frac=val_frac,\n",
    "    seed = None,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "\n",
    "plt.figure(figsize=(1.3*6.4, 4.8), dpi=125)\n",
    "\n",
    "plt.semilogy(list(range(1, epochs+1)), results[\"train_loss\"], label=\"Train\")\n",
    "plt.semilogy(list(range(1, epochs+1)), results[\"val_loss\"], label=\"Test\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Evolution of training and test losses\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, mask, sigma = dataset[:]\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    x_hat, y_hat = model.forward(x)\n",
    "\n",
    "# Conversion to NumPy\n",
    "x = x.detach().numpy()\n",
    "x_hat = x_hat.detach().numpy()\n",
    "y_hat = y_hat.detach().numpy()\n",
    "sigma = sigma.detach().numpy()\n",
    "\n",
    "# Reshaping\n",
    "X = x.T.reshape(nz, ny, nx)\n",
    "X_hat = x_hat.T.reshape(nz, ny, nx)\n",
    "Y_hat = y_hat.T.reshape(bottleneck_size, ny, nx)\n",
    "Sigma = sigma.reshape(ny, nx)\n",
    "\n",
    "# Residuals\n",
    "Residuals = X - X_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubes save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(f\"output_{line_name}\"):\n",
    "    os.mkdir(f\"output_{line_name}\")\n",
    "\n",
    "fits.PrimaryHDU(X, header).writeto(os.path.join(f\"output_{line_name}\", \"input.fits\"), overwrite=True)\n",
    "fits.PrimaryHDU(X_hat, header).writeto(os.path.join(f\"output_{line_name}\", \"output.fits\"), overwrite=True)\n",
    "fits.PrimaryHDU(Residuals, header).writeto(os.path.join(f\"output_{line_name}\", \"residuals.fits\"), overwrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"model_{line_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
